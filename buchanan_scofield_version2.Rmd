---
title             : "Bulletproof Bias? Considering the Type of Data in Common Proportion of Variance Effect Sizes"
shorttitle        : "Bulletproof Bias?"

author: 
  - name          : "Erin M. Buchanan"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "901 S National, Springfield, MO, 65897"
    email         : "erinbuchanan@missouristate.edu"
  - name          : "John E. Scofield"
    affiliation   : "2"

affiliation:
  - id            : "1"
    institution   : "Missouri State University"
  - id            : "2"
    institution   : "University of Missouri"

author_note: >
  Erin M. Buchanan, Department of Psychology, Missouri State University; John E. Scofield, Department of Psychological Sciences, University of Missouri. Both authors contributed equally to the creation of all parts of this manuscript. 

abstract: >
  As effect sizes gain ground as important indicators of practical significance and as a meta-analytic tool, we must critically understand their limitations and biases. This project investigates the positive bias of eta squared and critical suggestions of the alternative use of omega squared or epsilon for their mitigation of bias. Variance overlap effect sizes were examined for potential bias in different data scenarios (i.e., truncated and Likert type data) to elucidate differences in bias from previous research. We found that data precision and truncation/skew affected effect size bias, often lowering the bias in eta squared. This work expands our understanding of bias on variance overlap measures and allows researchers to make an informed choice about the type of effect to report given aspects of their research study. Implications for sample size planning and power are also discussed.  
  
keywords          : "bias, eta, omega, epsilon"

bibliography      : ["BiasOneRef.bib"]

figsintext        : no
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : no

lang              : "english"
class             : "man"
output            : papaja::apa6_pdf
---

```{r libraries, include = FALSE}
library("papaja")
library(MOTE)
##library source
library(ggplot2)
library(reshape)
cleanup = theme(panel.grid.major = element_blank(), 
                panel.grid.minor = element_blank(), 
                panel.background = element_blank(), 
                axis.line.x = element_line(color = "black"),
                axis.line.y = element_line(color = "black"),
                legend.key = element_rect(fill = "white"),
                text = element_text(size = 13))
```
##Null Hypothesis Significance Testing
  Null hypothesis significance testing (NHST) has dominated the social sciences for the past century, and currently, a blend of the Fisher and Neyman-Pearson methods are taught and practiced in psychological research [@Gigerenzer2004]. However, the sole use of NHST can be inadequate in terms of statistical inference [@Kirk2003]. Statistical reform is slow, hindering growth in the use of effect sizes [@Fritz2012; @Fritz2013], confidence intervals, or even error bars on graphs [@Cumming2005; @Cumming2007]. While a detailed discussion of the criticisms of NHST is outside the scope of this article [@Loftus1996; @Kruschke2010; @Wagenmakers2007], we briefly discuss a few key issues to highlight the necessity of alternative means of estimation, namely effect sizes and their confidence intervals. The assumption of the truth of null hypotheses is erroneous, as they can be falsified with large enough sample sizes [@Cohen1990; @Tukey1991]. Two comparisons in the real world are always, to some degree, different. A continual increase in sample size will eventually lead to the rejection of the null hypothesis. As @Cohen1990 says, "if the null hypothesis is always false, what's the big deal about rejecting it?" (pg. 1308).
  
  As @Kirk2003 describes, the use of NHST does not actually provide answers to the pivotal questions surrounding researchers today. NHST tells us the probability of obtaining observed, or more extreme data, assuming that the null hypothesis is true [@Kruschke2011]. NHST does not expound on the probability of a research hypothesis being correct, begetting a series of misinformed reject-this to accept-that type of reasoning for supporting research hypotheses. Another criticism of NHST lies with the encouragement of a dichotomous decision-making paradigm (i.e., reject, do not reject). *p*-values, a central component of dichotomous decision-making, are defined as the probability that data is equal to, or more extreme than its observed value, given the null hypothesis is true [@Cohen1990; @Wagenmakers2007]. However, *p*-values are often misunderstood and misused [@Cumming2014; @Gigerenzer2004]. *p*-values are sometimes assumed to indicate of the size or strength of an effect (i.e., suggesting that *p* < .001 is *very* significant), and questionable research practices, such as *p*-hacking [@Simmons2011] can lead to artificially significant results and Type I errors. Effects are dichotomized into significant and non-significant, without shedding light on the practical importance of findings. Parametric NHST is also sensitive to the violations of statistical assumptions, given the common occurrence of non-normal data in some phenomena [@Tabachnick2012]. Even with recent suggestions to move the $\alpha$ criterion to a lower threshold, such as *p* < .005 [@Benjamin2017], this shift does not ultimately solve inherent problems with *p*-values and misguided interpretations. Research hypotheses should not be merely centered on dichotomous decision criteria and *p*-values, but should consider the magnitude, or practical significance, of scientific phenomena [@Kirk2003; @Thompson2007].

##Effect Size
  Effect sizes are argued to be as, or more, important indicators of statistical inference than the sole reliance on NHST [@Cumming2014; @Rosenthal1994]. @Lakens2013 describes effect sizes as "the most important outcome of empirical studies" (pg. 1). Effect sizes add the ability to interpret the practical importance of a phenomenon, centered in the context of the research field, alongside or in place of statistical significance [@CohenJ1994]. Additionally, effect sizes are integral to meta-analyses and *a priori* power analysis planning. Even though @Wilkinson1999 and the newest style guide manuals have heavily suggested the use of effect sizes, @Fritz2012 and @Fritz2013 have shown that report rates of effect sizes are still low. The current literature shows a renewed interest in effect sizes, as Google Scholar searches indicate a slew of publications on effect size and meta-analysis in recent years. While effect sizes have been suggested as an integral tool of statistical inference, specific characteristics of multiple effect sizes are still under scrutiny, with @Kirk2003 noting the development of more than seventy separate effect size indices.
  
  The majority of effect size estimates can be binned into two families. The *d* family is generally based on mean differences, whereas the *r* family is based on the association strength of proportion of variance explained by manipulated factors [@Maxwell2004; @Kelley2012]. This article will focus on proportion of variance effect sizes, specifically, as previous research on the distribution of the *d* family has inspired this work [@Smithson2001; @Cumming2014]. Eta squared ($\eta^2$), a proportion of variance effect size, is the most common effect size reported alongside ANOVAs, even though $R^2$ utilizes the same formula [@Grissom2005; @Matsumoto2011]. $\eta^2$ is calculated from the sum of squares associated with the effect and the sum of squares associated with the error term in a sample, which is interpreted as a point-estimate for the population [@Cumming2014; @Pearson1905].
  
  Effect sizes, however, are not all created equally, as sample-based effect sizes can overestimate population effects. @Maxwell2004 describe this bias as the likelihood of sample means inevitably deviating from population means, due to sampling variability. The positive bias of $\eta^2$ has been widely recognized, due to its reliance with least-squares methods associated with *t*-tests and ANOVA designs, as well as with sampling error variability across different studies [@Thompson2007; @Snyder1993; @Stevens1992; @Yin2001; @Skidmore2011; @Skidmore2012; @Shieh2008; @Thompson2006]. Multiple bias-correcting formulas have been established to help "shrink", or adjust, the bias from the population and sample effect size estimates [@Olkin1958; @Stuart1994; @Raju1999], and @Yin2001 provide detailed discussion of these formulas and their implementation. Bias-correcting formulas developed for $\eta^2$ are not examined further here because these formulas correct for a general assumption of positive bias, whereas positive bias was a central question to our current investigation.
  
  Epsilon squared ($\epsilon^2$) was proposed by @Kelley1935, offering an alternative, less biased effect size measure. $\epsilon^2$ adjusts for positive bias by correcting the numerator in the formula of $\eta^2$. This correction occurs by subtracting the mean square error from the sum of squares treatment [@Olejnik2000]. Omega squared ($\omega^2$), introduced by @Hays1963, takes this adjustment a step further, by not only subtracting the mean square error in the numerator, but subtracting the mean square error from the sum of squares total in the denominator as well [@Olejnik2000]. @Carroll1975, however, noted that $\epsilon^2$ and $\omega^2$ yield similar ranges of effect sizes. While the use of $\omega^2$ and $\epsilon^2$ can reduce sampling variability evident in $\eta^2$, some have argued that these effect size adjustments do not eliminate bias entirely [@Keselman1975; @Olejnik2000; @Maxwell2004; @Okada2013; @Okada2016]. @Maxwell2004 note that typical differences between $\omega^2$ and $\eta^2$ estimates fall within .02 of each other, while @OGrady1982 argued that $\omega^2$ may also occasionally lead to an underestimation of population effect size estimates.

  $\omega^2$, like every other effect size estimate, is not without its limitations. First, $\omega^2$ may be computationally difficult to calculate, especially in complex designs, and even textbooks that discuss $\omega^2$ detail this limitation [@Field2012]. For meta-analytic purposes, $\omega^2$ generally cannot be computed because of reliance on sum of squares and mean squared error values, which have no translatable estimate formula from the *F*-statistic like $\eta^2$. Using $\omega^2$ may be inappropriate in designs where sample sizes across groups are unequal, as the model in which $\omega^2$ was derived assumes a balanced design [@Vaughan1969; @Olejnik2000]. However, @Carroll1975 noted that $\omega^2$ estimations with unequal *n* negligibly affected estimates, unless combined with heterogeneous variances. 

##Overall Limitations and Bias
  Each effect size includes aspects in regards to its use in scientific research, as well as limitations in its interpretation. This paper focuses on the bias, standard deviation, and standard error of each effect to expand on previous research by @Okada2013. @Fern1996 outlined that proportion of variance effect sizes should be interpreted depending on the type of research design used, whether treatments were manipulated between- or within-subjects, and whether experiments contained more than one factor. Researchers should consider elements, such as measurement reliability, heterogeneity of variance, the range of treatment manipulations, and the type of scale used before assigning a value of importance to a given effect size estimate. Large sample sizes increase accuracy of effect size estimation, as smaller sample sizes are associated with larger standard errors. This limitation is especially prevalent with the combination of small sample sizes and heterogeneous variances [@Carroll1975]. Others have commented that sample-based estimates have higher sampling errors with small *n* [@Thompson2007; @Maxwell2004], and that differences in effect sizes, such as $\eta^2$, $\omega^2$, and $\epsilon^2$, tend to converge as sample size substantially increases [@Maxwell1981].
  
  Several previous studies have examined the interplay between bias, standard deviation, and standard error, mostly focusing on sample size and effect size as variables of interest. @Keselman1975 ran 1,000 simulations of ANOVAs and determined that $\omega^2$ estimates were closer to the population effect than both $\epsilon^2$ and $\eta^2$. $\omega^2$ has been advocated as the "nearly unbiased" estimator of the population effect, wherein $\omega^2$ has less bias than $\epsilon^2$, leaving $\eta^2$ as the most biased [@Grissom2005; @Matsumoto2011]. However, $\omega^2$ can be negatively (i.e., underestimate) biased, as originally found in @Carroll1975 and replicated in @OGrady1982. @Okada2013 extended these results by comparing all three effect sizes across sample size and mean variability by simulating 1,000,000 one-way fixed effect ANOVAs with four treatment levels. While Okada used 1,000,000 simulations, many researchers have concluded that far fewer simulations can achieve an adequate attenuation of sampling variability [@Fan2001; @Robey1992; @Skidmore2012].
  
  Results from @Okada2013 revealed that, $\epsilon^2$, not $\omega^2$, held the least bias across *n*. $\omega^2$ did, however, outperform both $\epsilon^2$ and $\eta^2$ in standard error. $\eta^2$ did have the lowest standard deviation, but Okada notes that $\eta^2$ performed poorly considering both bias and standard error. Okada concluded with a firm cautionary warning that researchers should not use $\eta^2$, as it incorrectly overestimates the population effect across sample, inversely related to sample size. @Skidmore2012 also investigated the performance of these effect sizes, considering common statistical assumption violations like heterogeneity, and found similar results as previous simulation studies [@Okada2013; @Carroll1975; @Keselman1975]. Even though these studies have focused on slightly different facets of bias in one-way between-subjects ANOVA, a common theme has emerged. Even with bias decreasing as an additive function of both an increase in population effect size and sample size [@Maxwell2004], conclusions from these studies remain the same: $\eta^2$ should not be used. This conclusion is intriguing, largely because $\eta^2$ is readily presented among statistical software packages (e.g. SPSS, R, JASP) and continues to be the most commonly reported effect size for ANOVA type designs [@Lakens2013].

##Purpose of Current Study
  The purpose of the current study is to examine these important conclusions regarding the use of $\eta^2$, as well as examining the type of data used in research design and its effects on the performance of $\eta^2$, $\epsilon^2$, and $\omega^2$. While former studies have investigated cases when ANOVA model assumptions hold true, we investigated the case when aspects do not hold true, which is often reflected in cases of real psychological data analysis.
  
We first established empirical confidence intervals of bias, standard deviation, and standard error to use as a criterion for judging changes across our study. Second, we examined the effect of data type on bias, standard deviation, and standard error. Last, for the data type manipulations that demonstrably change bias, standard deviation, or standard error, we calculated this change across a large range of effect sizes (i.e., nearly zero to very large). This analysis demonstrates the effect of the bias, standard deviation, or standard error change with more precision than previous work binning effect sizes into small, medium, or large magnitudes. Bias was then use to demonstrate the effect of social sciences' heavy reliance on $\eta^2$ when sample size planning for an appropriately powered study. 

# Method

## Simulations
  To further the work by @Okada2013, it is first advantageous to explain the underlying assumptions and formulas used in this extension. While these formulas can be found in many introductory statistics textbooks covering ANOVA methods, we describe them below as well as provide *R* syntax online to aid in the reproducibility and understanding of the current extension. Population effect size is calculated by creating a sum of squares for between group differences from the grand mean:
  
$$
SS_{between} = \frac{\Sigma (\bar{X_{i}} - \bar{X})^2} {N_{groups}}
$$
and then determining group error with an average of group variances. In this formula, $\bar{X_{i}}$ indicates each proposed group mean, while $\bar{X}$ is the proposed grand average of group means. Population eta is then calculated by creating a ratio of $SS_{between}$ and $SS_{error}$, analogous to the full $\eta^2$ formula:

$$
\eta^2_{pop} = \frac{SS_{between}} {SS_{between} + SS_{error}}.
$$
$SS_{error}$ is calculated by averaging all proposed group variances. Specifically, we used standard deviations of one to match previous simulations, but this point for calculation of $\eta^2_{pop}$ from *R* syntax should be clarified for interested parties (i.e., the previous syntax included a value of one, while this value is calculated from group variances and should not be a set number). The value of one in the denominator is an expected average of group variances, which is approximately one in these examples. This value should be changed for simulations with heterogeneity or other proposed standard deviations. 

  Bias is estimated by calculating the average simulated $\eta^2$ values and subtracting the $\eta^2_{pop}$ estimate. This value represents the effect size deviation from the population, wherein positive numbers represent an inflation of effect size due to manipulated variables, while negative numbers indicate a reduction in effect size estimates, compared to population values. Standard deviation (SD) and root mean squared error (standard error; RMSE) of the effect size are simple adjustments of the traditional standard deviation formula:
  
$$
SD = \sqrt\frac{\Sigma (\eta^2_{i} - \bar{\eta^2})^2} {N_{sims}}
$$
and:

$$
RMSE = \sqrt\frac{\Sigma (\eta^2_{i} - \eta^2_{pop})^2} {N_{sims}}.
$$
Here, sample SD is calculated by squaring the difference between the estimated $\eta^2_{i}$ for each simulation from the mean $\bar{\eta^2}$ of that sample size. RMSE estimates the standard error by squaring the differences of estimated $\eta^2_{i}$ for each simulation from the $\eta^2_{pop}$. Both formulas use the population sample size estimator ($N_{sims}$) in the denominator, rather than degrees of freedom (i.e., $N_{sims}$ - 1). This formula is consistent with previous work by @Okada2013 and is appropriate for simulation studies (i.e., with large simulation numbers, results are consistent with *N* or *N*-1).

## Manipulated Variables
  Using the cited simulation code, we manipulated several factors independently to determine the effects of data type on bias, SD, and RMSE of variance overlap effect sizes. Methodologically, we examined the number of simulations first in *R* to create confidence intervals as a comparison metric, described below. For data types, we varied the number of decimals, type of means, and truncation/skew of the data. In all simulations testing these variables, we used the largest variability condition from @Okada2013 (i.e., means 0.0, 0.0, 1.2, 1.2) because it showed the largest bias, SD, and RMSE effects. Each simulation was tested across sample size values and effect size types found in @Okada2013. The *R* code from our study can be found at http://osf.io/urd8q in an *R*markdown file that shows the simulation and graph code inline with this manuscript.  
 
# Results
##Confidence Intervals
```{r confidence interval creation, eval=FALSE, include=FALSE}
fullbias = 1:3
fullrmse = 1:3
fullsd = 1:3

for(i in 1:100){
  
muvec <- c(0.00,0.00,1.20,1.20)
meanmu <- mean(muvec)
sigb <- sum((muvec-meanmu)^2)/4
eta2p <- sigb/(sigb+1)
k <- length(muvec)
nsim <- 1000
njs <- c(10,20,30,40,50,60,70,80,90,100)
BIASmat <- matrix(NA,nrow=length(njs),ncol=3)
rownames(BIASmat) <- njs
colnames(BIASmat) <- c("eta2","epsilon2","omega2")
RMSEmat <- matrix(NA,nrow=length(njs),ncol=3)
rownames(RMSEmat) <- njs
colnames(RMSEmat) <- c("eta2","epsilon2","omega2")
SDmat <- matrix(NA,nrow=length(njs),ncol=3)
rownames(SDmat) <- njs
colnames(SDmat) <- c("eta2","epsilon2","omega2")
niter <- 1

for (nj in njs){
  x <- matrix(NA,nrow=nj,ncol=4)
  eta2 <- rep(NA,nsim)
  epsilon2 <- rep(NA,nsim)
  omega2 <- rep(NA,nsim)
  for (ii in 1: nsim){
    y <- c(rnorm(n=nj,mean=muvec[1],sd=1),
           rnorm(n=nj,mean=muvec[2],sd=1),
           rnorm(n=nj,mean=muvec[3],sd=1),
           rnorm(n=nj,mean=muvec[4],sd=1))
    x <- as.factor(c(rep("mu1",nj),rep("mu2",nj),
                     rep("mu3",nj),rep("mu4",nj)))
    res <- anova(aov(y~x))
    res <- as.matrix(res)
    SSb <- res[1,2]
    SSt <- res[1,2] + res[2,2]
    MSw <- res[2,3]
    eta2[ii] <- SSb/SSt
    epsilon2[ii] <- (SSb - 3*MSw)/SSt
    omega2[ii] <- (SSb - 3*MSw)/(SSt+MSw)
  }  
  BIASmat[niter,1] <- mean(eta2) - eta2p
  BIASmat[niter,2] <- mean(epsilon2) - eta2p
  BIASmat[niter,3] <- mean(omega2) - eta2p
  RMSEmat[niter,1] <- sqrt(sum((eta2-eta2p)^2)/nsim)
  RMSEmat[niter,2] <- sqrt(sum((epsilon2-eta2p)^2)/nsim)
  RMSEmat[niter,3] <- sqrt(sum((omega2-eta2p)^2)/nsim)
  SDmat[niter,1] <- sqrt(sum((eta2-mean(eta2))^2)/nsim)
  SDmat[niter,2] <- sqrt(sum((epsilon2-mean(epsilon2))^2)
                         /nsim)
  SDmat[niter,3] <- sqrt(sum((omega2-mean(omega2))^2)
                         /nsim)
  niter <- niter+1
}

fullbias = rbind(fullbias, BIASmat)
fullrmse = rbind(fullrmse, RMSEmat)
fullsd = rbind(fullsd, SDmat)
}

write.csv(fullbias, "biasCI.csv")
write.csv(fullrmse, "rmseCI.csv")
write.csv(fullsd, "sdCI.csv")
```
  1,000 simulations per condition was used for this study. This option can be practical for researchers in terms of computation time, as results using 1,000 simulations are remarkably similar to @Okada2013. From one hundred iterations of 1,000 simulations, empirical 95% confidence intervals were calculated by taking the 2.5% and 97.5% values (i.e., values ranked 2 and 3 were averaged or values 97 and 98 were averaged). Confidence intervals are very small around average scores, indicating a reasonable range of values to be found. Graphs depicting this finding can be found online at http://osf.io/urd8q. The empirical confidence interval calculated here was used to determine how bias, SD, and RMSE changed across data type manipulations.
```{r simulation-graph, include=FALSE, fig.cap="Replication of Okada (2013) bias, SD, and RMSE for the largest variability condition. Effect size values are presented for average simulations alongside Okada scores. Shaded gray area represents 95% confidence interval around these values.", fig.height=6, fig.width=6}

##import files
biasci = read.csv("biasCI.csv")
rmseci = read.csv("rmseCI.csv")
sdci = read.csv("sdCI.csv")

####sort the datasets by N otherwise rank code doesn't work####
biasci = biasci[order(biasci$N),]
rmseci = rmseci[order(rmseci$N),]
sdci = sdci[order(sdci$N),]

####BIAS####
##order N values
biasci$ordereta = unlist(with(biasci, tapply(eta2, N, rank)))
biasci$orderep = unlist(with(biasci, tapply(epsilon2, N, rank)))
biasci$orderom = unlist(with(biasci, tapply(omega2, N, rank)))

##figure out the 95% CI by using average scores
highbiaseta = subset(biasci, ordereta == 98 | ordereta == 97)
hibeta = with(highbiaseta, tapply(eta2, N, mean))
lowbiaseta = subset(biasci, ordereta == 3 | ordereta == 2)
lobeta = with(lowbiaseta, tapply(eta2, N, mean))

highbiasep = subset(biasci, orderep == 98 | orderep == 97)
hibep = with(highbiasep, tapply(epsilon2, N, mean))
lowbiasep = subset(biasci, orderep == 3 | orderep == 2)
lobep = with(lowbiasep, tapply(epsilon2, N, mean))

highbiasom = subset(biasci, orderom == 98 | orderom == 97)
hibom = with(highbiasom, tapply(omega2, N, mean))
lowbiasom = subset(biasci, orderom == 3 | orderom == 2)
lobom = with(lowbiasom, tapply(omega2, N, mean))

##average and previous bias
avgbiaseta = with(biasci, tapply(eta2, N, mean))
avgbiasep = with(biasci, tapply(epsilon2, N, mean))
avgbiasom = with(biasci, tapply(omega2, N, mean))

obiaseta = c(.054, .027, .018, .013, .011, .009, .008, .007, .006, .005)
obiasep = c(-.002, -.001, -.001, -.001, -.001, .000, .000, .000, .000, .000)
obiasom = c(-.007, -.004, -.002, -.002, -.001, -.001, -.001, -.001, -.001, -.001)

bgraphdata = as.data.frame(rbind(avgbiaseta, avgbiasep, avgbiasom, obiaseta, obiasep, obiasom))
bgraphdata$Type = c("eta", "epsilon", "omega", "eta", "epsilon", "omega")
bgraphdata$source = c("BS", "BS", "BS", "OK", "OK", "OK")
bgraphdatalong = melt(bgraphdata, id = c("Type", "source"))
bgraphdatalong$variable = as.numeric(bgraphdatalong$variable) * 10
bgraphdatalong$HICI = melt(unname(rbind(hibeta, hibep, hibom, hibeta, hibep, hibom)))$value
bgraphdatalong$LOCI = melt(unname(rbind(lobeta, lobep, lobom, lobeta, lobep, lobom)))$value

biasline = 
  ggplot(bgraphdatalong, aes(variable, value, 
                             color = Type, 
                             group = interaction(Type, source))) +
  geom_line(size=.7) +
  geom_ribbon(aes(ymin = LOCI, 
                  ymax = HICI, 
                  group = Type), 
              fill = "grey70",
              color = FALSE,
              alpha = .4) +
  coord_cartesian(ylim = c(-.02, .14)) +
  ylab("Bias") +
  xlab("Sample Size") + 
  geom_abline(slope = 0, intercept = 0) +
  scale_color_manual(name = "Effect Size", 
                     labels = c("Epsilon", "Eta", "Omega"),
                     values = c("darkgrey", "black", "grey20")) +
  cleanup

####RMSE####
##order N values
rmseci$ordereta = unlist(with(rmseci, tapply(eta2, N, rank)))
rmseci$orderep = unlist(with(rmseci, tapply(epsilon2, N, rank)))
rmseci$orderom = unlist(with(rmseci, tapply(omega2, N, rank)))

##figure out the 95% CI by using average scores
highrmseeta = subset(rmseci, ordereta == 98 | ordereta == 97)
hireta = with(highrmseeta, tapply(eta2, N, mean))
lowrmseeta = subset(rmseci, ordereta == 3 | ordereta == 2)
loreta = with(lowrmseeta, tapply(eta2, N, mean))

highrmseep = subset(rmseci, orderep == 98 | orderep == 97)
hirep = with(highrmseep, tapply(epsilon2, N, mean))
lowrmseep = subset(rmseci, orderep == 3 | orderep == 2)
lorep = with(lowrmseep, tapply(epsilon2, N, mean))

highrmseom = subset(rmseci, orderom == 98 | orderom == 97)
hirom = with(highrmseom, tapply(omega2, N, mean))
lowrmseom = subset(rmseci, orderom == 3 | orderom == 2)
lorom = with(lowrmseom, tapply(omega2, N, mean))

##average and previous rmse
avgrmseeta = with(rmseci, tapply(eta2, N, mean))
avgrmseep = with(rmseci, tapply(epsilon2, N, mean))
avgrmseom = with(rmseci, tapply(omega2, N, mean))

ormseeta = c(.123, .083, .067, .057, .051, .046, .043, .040, .038, .036)
ormseep = c(.120, .082, .066, .057, .051, .046, .043, .040, .037, .036)
ormseom = c(.120, .081, .066, .057, .050, .046, .043, .040, .038, .035)

bgraphdata2 = as.data.frame(rbind(avgrmseeta, avgrmseep, avgrmseom, ormseeta, ormseep, ormseom))
bgraphdata2$Type = c("eta", "epsilon", "omega", "eta", "epsilon", "omega")
bgraphdata2$source = c("BS", "BS", "BS", "OK", "OK", "OK")
bgraphdatalong2 = melt(bgraphdata2, id = c("Type", "source"))
bgraphdatalong2$variable = as.numeric(bgraphdatalong2$variable) * 10
bgraphdatalong2$HICI = melt(unname(rbind(hireta, hirep, hirom, hireta, hirep, hirom)))$value
bgraphdatalong2$LOCI = melt(unname(rbind(loreta, lorep, lorom, loreta, lorep, lorom)))$value

rmseline = 
  ggplot(bgraphdatalong2, aes(variable, value, 
                              color = Type, 
                              group = interaction(Type, source))) +
  geom_line(size=.7) +
  geom_ribbon(aes(ymin = LOCI, 
                  ymax = HICI, 
                  group = Type), 
              fill = "grey70",
              color = FALSE,
              alpha = .4) +
  coord_cartesian(ylim = c(-.02, .14)) +
  ylab("RMSE") +
  xlab("Sample Size") + 
  geom_abline(slope = 0, intercept = 0) +
  scale_color_manual(values = c("darkgrey", "black", "grey20")) +
  cleanup


####SD####
##order N values
sdci$ordereta = unlist(with(sdci, tapply(eta2, N, rank)))
sdci$orderep = unlist(with(sdci, tapply(epsilon2, N, rank)))
sdci$orderom = unlist(with(sdci, tapply(omega2, N, rank)))

##figure out the 95% CI by using average scores
highsdeta = subset(sdci, ordereta == 98 | ordereta == 97)
hiseta = with(highsdeta, tapply(eta2, N, mean))
lowsdeta = subset(sdci, ordereta == 3 | ordereta == 2)
loseta = with(lowsdeta, tapply(eta2, N, mean))

highsdep = subset(sdci, orderep == 98 | orderep == 97)
hisep = with(highsdep, tapply(epsilon2, N, mean))
lowsdep = subset(sdci, orderep == 3 | orderep == 2)
losep = with(lowsdep, tapply(epsilon2, N, mean))

highsdom = subset(sdci, orderom == 98 | orderom == 97)
hisom = with(highsdom, tapply(omega2, N, mean))
lowsdom = subset(sdci, orderom == 3 | orderom == 2)
losom = with(lowsdom, tapply(omega2, N, mean))

##average and previous sd
avgsdeta = with(sdci, tapply(eta2, N, mean))
avgsdep = with(sdci, tapply(epsilon2, N, mean))
avgsdom = with(sdci, tapply(omega2, N, mean))

osdeta = c(.111, .079, .064, .056, .050, .045, .042, .039, .037, .035)
osdep = c(.120, .082, .066, .057, .051, .046, .043, .040, .037, .036)
osdom = c(.119, .081, .066, .057, .050, .046, .043, .040, .037, .035)

bgraphdata3 = as.data.frame(rbind(avgsdeta, avgsdep, avgsdom, osdeta, osdep, osdom))
bgraphdata3$Type = c("eta", "epsilon", "omega", "eta", "epsilon", "omega")
bgraphdata3$source = c("BS", "BS", "BS", "OK", "OK", "OK")
bgraphdatalong3 = melt(bgraphdata3, id = c("Type", "source"))
bgraphdatalong3$variable = as.numeric(bgraphdatalong3$variable) * 10
bgraphdatalong3$HICI = melt(unname(rbind(hiseta, hisep, hisom, hiseta, hisep, hisom)))$value
bgraphdatalong3$LOCI = melt(unname(rbind(loseta, losep, losom, loseta, losep, losom)))$value

sdline = 
  ggplot(bgraphdatalong3, aes(variable, value, 
                              color = Type, 
                              group = interaction(Type, source))) +
  geom_line(size=.7) +
  geom_ribbon(aes(ymin = LOCI, 
                  ymax = HICI, 
                  group = Type), 
              fill = "grey70",
              color = FALSE,
              alpha = .4) +
  coord_cartesian(ylim = c(-.02, .14)) +
  ylab("SD") +
  xlab("Sample Size") + 
  geom_abline(slope = 0, intercept = 0) +
  scale_color_manual(values = c("darkgrey", "black", "grey20")) +
  cleanup

# arrange plots together
library(cowplot)
legend = get_legend(biasline)
prow <- plot_grid( biasline + theme(legend.position="none"),
                   rmseline + theme(legend.position="none"),
                   sdline + theme(legend.position="none"),
                   legend,
                   hjust = -1,
                   nrow = 2
)
prow
```

## Precision of the Dependent Variable
```{r full-rounding, eval=FALSE, include=FALSE}
####rounding to whole numbers across decimals####
rmsefunction = function (x) { sqrt(sum((x-eta2p)^2)/nsim) }
sdfunction = function(x) { sqrt(sum((x-mean(x))^2)/nsim) }
muvec <- c(0.00,0.00,1.20,1.20)
meanmu <- mean(muvec)
sigb <- sum((muvec-meanmu)^2)/4
eta2p <- sigb/(sigb+1)
k <- length(muvec)
nsim <- 1000
njs <- c(10,20,30,40,50,60,70,80,90,100)
BIASmat <- matrix(NA,nrow=length(njs),ncol=48)
rownames(BIASmat) <- njs
#colnames(BIASmat) <- c("eta2","epsilon2","omega2")
RMSEmat <- matrix(NA,nrow=length(njs),ncol=48)
rownames(RMSEmat) <- njs
#colnames(RMSEmat) <- c("eta2","epsilon2","omega2")
SDmat <- matrix(NA,nrow=length(njs),ncol=48)
rownames(SDmat) <- njs
#colnames(SDmat) <- c("eta2","epsilon2","omega2")
niter <- 1

for (nj in njs){
  x <- matrix(NA,nrow=nj,ncol=4)
  eta2 <- matrix(NA,nrow=nsim,ncol=16)
  epsilon2 <- matrix(NA,nrow=nsim,ncol=16)
  omega2 <- matrix(NA,nrow=nsim,ncol=16)
  for (ii in 1: nsim){
    y <- c(rnorm(n=nj,mean=muvec[1],sd=1),
           rnorm(n=nj,mean=muvec[2],sd=1),
           rnorm(n=nj,mean=muvec[3],sd=1),
           rnorm(n=nj,mean=muvec[4],sd=1))
    
    for (iii in 16:1) {
    
    y = round(y, digits = (iii-1))
    
    x <- as.factor(c(rep("mu1",nj),rep("mu2",nj),
                     rep("mu3",nj),rep("mu4",nj)))
    
    res <- anova(aov(y~x))
    res <- as.matrix(res)
    SSb <- res[1,2]
    SSt <- res[1,2] + res[2,2]
    MSw <- res[2,3]
    
    eta2[ii,iii] <- SSb/SSt
    epsilon2[ii,iii] <- (SSb - 3*MSw)/SSt
    omega2[ii,iii] <- (SSb - 3*MSw)/(SSt+MSw)
    
    }
  }  
  BIASmat[niter,1:16] <- apply(eta2, 2, mean) - eta2p
  BIASmat[niter,17:32] <- apply(epsilon2, 2, mean) - eta2p
  BIASmat[niter,33:48] <- apply(omega2, 2, mean) - eta2p
  RMSEmat[niter,1:16] <- apply(eta2, 2, rmsefunction)
  RMSEmat[niter,17:32] <- apply(epsilon2, 2, rmsefunction)
  RMSEmat[niter,33:48] <- apply(omega2, 2, rmsefunction)
  SDmat[niter,1:16] <- apply(eta2, 2, sdfunction)
  SDmat[niter,17:32] <- apply(epsilon2, 2, sdfunction)
  SDmat[niter,33:48] <- apply(omega2, 2, sdfunction)
  niter <- niter+1
}

write.csv(cbind(BIASmat, RMSEmat, SDmat), "rounding1_16.csv")
```

```{r round0v1, eval=FALSE, include=FALSE}
####rounding to whole numbers####
muvec <- c(0.00,0.00,1.20,1.20)
meanmu <- mean(muvec)
sigb <- sum((muvec-meanmu)^2)/4
eta2p <- sigb/(sigb+1)
k <- length(muvec)
nsim <- 1000
njs <- c(10,20,30,40,50,60,70,80,90,100)
BIASmat <- matrix(NA,nrow=length(njs),ncol=3)
rownames(BIASmat) <- njs
colnames(BIASmat) <- c("eta2","epsilon2","omega2")
RMSEmat <- matrix(NA,nrow=length(njs),ncol=3)
rownames(RMSEmat) <- njs
colnames(RMSEmat) <- c("eta2","epsilon2","omega2")
SDmat <- matrix(NA,nrow=length(njs),ncol=3)
rownames(SDmat) <- njs
colnames(SDmat) <- c("eta2","epsilon2","omega2")
niter <- 1

for (nj in njs){
  x <- matrix(NA,nrow=nj,ncol=4)
  eta2 <- rep(NA,nsim)
  epsilon2 <- rep(NA,nsim)
  omega2 <- rep(NA,nsim)
  for (ii in 1: nsim){
    y <- c(rnorm(n=nj,mean=muvec[1],sd=1),
           rnorm(n=nj,mean=muvec[2],sd=1),
           rnorm(n=nj,mean=muvec[3],sd=1),
           rnorm(n=nj,mean=muvec[4],sd=1))
    
    y = round(y, digits = 0)
    
    x <- as.factor(c(rep("mu1",nj),rep("mu2",nj),
                     rep("mu3",nj),rep("mu4",nj)))
    
    res <- anova(aov(y~x))
    res <- as.matrix(res)
    SSb <- res[1,2]
    SSt <- res[1,2] + res[2,2]
    MSw <- res[2,3]
    
    eta2[ii] <- SSb/SSt
    epsilon2[ii] <- (SSb - 3*MSw)/SSt
    omega2[ii] <- (SSb - 3*MSw)/(SSt+MSw)
  }  
  BIASmat[niter,1] <- mean(eta2) - eta2p
  BIASmat[niter,2] <- mean(epsilon2) - eta2p
  BIASmat[niter,3] <- mean(omega2) - eta2p
  RMSEmat[niter,1] <- sqrt(sum((eta2-eta2p)^2)/nsim)
  RMSEmat[niter,2] <- sqrt(sum((epsilon2-eta2p)^2)/nsim)
  RMSEmat[niter,3] <- sqrt(sum((omega2-eta2p)^2)/nsim)
  SDmat[niter,1] <- sqrt(sum((eta2-mean(eta2))^2)/nsim)
  SDmat[niter,2] <- sqrt(sum((epsilon2-mean(epsilon2))^2)
                         /nsim)
  SDmat[niter,3] <- sqrt(sum((omega2-mean(omega2))^2)
                         /nsim)
  niter <- niter+1
}

write.csv(cbind(BIASmat, RMSEmat, SDmat), "rounding.csv")

```

  We then explored the effect of the type of dependent variable on bias, RMSE, and SD, starting with the precision of the dependent variable. *rnorm()* calculates random numbers to 16 decimal places. With computer assisted data collection, some phenomena may be calculated with more precision (i.e., more decimals), but it is unlikely that all data is collected with a large number of significant digits. Additionally, if researchers use Likert type data, it is possible that dependent variables may be captured in whole numbers. We simulated 1,000 datasets and within each round, we calculated bias, RMSE, and SD on the same data, rounding values from 0 to 15 decimals. After examining these data, only very small changes were apparent through one decimal (i.e., 1-15 decimals do not show differences larger than .001), and therefore, we only present results from rounding to the least precision, zero decimals. The complete dataset and code for both of these simulations are presented online at http://osf.io/urd8q. 
  
```{r decimal-graph, echo=FALSE, fig.cap = "Bias, SD, and RMSE values of zero decimal dependent variables. Shaded gray area represents 95% confidence interval around these values.", fig.height=8, fig.width=8}
round = read.csv("rounding.csv")

####bias graph####
bgraphdatalong = melt(round[ , 1:4],
                      id = "N",
                      measured = c("eta2B", "epsilon2B", "omega2B"))
bgraphdatalong$variable = factor(bgraphdatalong$variable, 
                                 levels = c("epsilon2B", "eta2B", "omega2B"))
bgraphdatalong$HICI = melt(unname(c(hibeta, hibep, hibom)))$value
bgraphdatalong$LOCI = melt(unname(c(lobeta, lobep, lobom)))$value

biasline = 
  ggplot(bgraphdatalong, aes(N, value, 
                             color = variable,
                             group = variable)) +
  geom_line(size=.7, aes(linetype = variable)) +
  geom_ribbon(aes(ymin = LOCI, 
                  ymax = HICI, 
                  group = variable), 
              fill = "grey70",
              color = FALSE,
              alpha = .4) +
  coord_cartesian(ylim = c(-.02, .14)) +
  ylab("Bias") +
  xlab("Sample Size") + 
  geom_abline(slope = 0, intercept = 0) +
  scale_color_manual(name = "Effect Size", 
                     labels = c("Epsilon", "Eta", "Omega"),
                     values = c("grey20", "grey20", "grey70")) +
  scale_linetype(name = "Effect Size", 
                 labels = c("Epsilon", "Eta", "Omega")) +
  cleanup

####rmse graph####
bgraphdatalong2 = melt(round[ , c(1, 5:7)],
                      id = "N",
                      measured = c("epsilon2R", "eta2R", "omega2R"))
bgraphdatalong2$variable = factor(bgraphdatalong2$variable, 
                                 levels = c("epsilon2R", "eta2R", "omega2R"))
bgraphdatalong2$HICI = melt(unname(c(hireta, hirep, hirom)))$value
bgraphdatalong2$LOCI = melt(unname(c(loreta, lorep, lorom)))$value

rmseline = 
  ggplot(bgraphdatalong2, aes(N, value, 
                             color = variable,
                             group = variable)) +
  geom_line(size=.7, aes(linetype = variable)) +
  geom_ribbon(aes(ymin = LOCI, 
                  ymax = HICI, 
                  group = variable), 
              fill = "grey70",
              color = FALSE,
              alpha = .4) +
  coord_cartesian(ylim = c(-.02, .14)) +
  ylab("RMSE") +
  xlab("Sample Size") + 
  geom_abline(slope = 0, intercept = 0) +
  scale_color_manual(name = "Effect Size", 
                     labels = c("Epsilon", "Eta", "Omega"),
                     values = c("grey20", "grey20", "grey70")) +
  scale_linetype(name = "Effect Size", 
                 labels = c("Epsilon", "Eta", "Omega")) +
  cleanup


####sd graph####
bgraphdatalong3 = melt(round[ , c(1, 8:10)],
                      id = "N",
                      measured = c("epsilon2S", "eta2S", "omega2S"))
bgraphdatalong3$variable = factor(bgraphdatalong3$variable, 
                                 levels = c("epsilon2S", "eta2S", "omega2S"))
bgraphdatalong3$HICI = melt(unname(c(hiseta, hisep, hisom)))$value
bgraphdatalong3$LOCI = melt(unname(c(loseta, losep, losom)))$value

sdline = 
  ggplot(bgraphdatalong3, aes(N, value, 
                             color = variable,
                             group = variable)) +
  geom_line(size=.7, aes(linetype = variable)) +
  geom_ribbon(aes(ymin = LOCI, 
                  ymax = HICI, 
                  group = variable), 
              fill = "grey70",
              color = FALSE,
              alpha = .4) +
  coord_cartesian(ylim = c(-.02, .14)) +
  ylab("SD") +
  xlab("Sample Size") + 
  geom_abline(slope = 0, intercept = 0) +
  scale_color_manual(name = "Effect Size", 
                     labels = c("Epsilon", "Eta", "Omega"),
                     values = c("grey20", "grey20", "grey70")) +
  scale_linetype(name = "Effect Size", 
                 labels = c("Epsilon", "Eta", "Omega")) +
  cleanup

# arrange plots together
library(cowplot)
legend = get_legend(biasline)
prow <- plot_grid( biasline + theme(legend.position="none"),
                   rmseline + theme(legend.position="none"),
                   sdline + theme(legend.position="none"),
                   legend,
                   hjust = -1,
                   nrow = 2
)
prow

bgraphdatalong$outhi = bgraphdatalong$value - bgraphdatalong$HICI
bgraphdatalong$outlo = bgraphdatalong$value - bgraphdatalong$LOCI
avgbiaslo =  round(mean(subset(bgraphdatalong$outlo, bgraphdatalong$outlo < 0)), 3)

bgraphdatalong2$outhi = bgraphdatalong2$value - bgraphdatalong2$HICI
bgraphdatalong2$outlo = bgraphdatalong2$value - bgraphdatalong2$LOCI
avgrmsehi =  round(mean(subset(bgraphdatalong2$outhi, bgraphdatalong2$outhi > 0)), 3)

bgraphdatalong3$outhi = bgraphdatalong3$value - bgraphdatalong3$HICI
bgraphdatalong3$outlo = bgraphdatalong3$value - bgraphdatalong3$LOCI
```

  As seen in Figure \@ref(fig:decimal-graph), rounding the dependent variable to zero decimals had a stark effect on the bias found in each of the effect sizes. All three were downwardly affected, that is, using less precise dependent variables resulted in less upwardly biased effect sizes. In fact, both $\omega^2$ and $\epsilon^2$ were completely negatively biased, and $\eta^2$ showed a negative bias after sample sizes approximately *n* = 40. These values average `r format(abs(avgbiaslo), nsmall = 3)` outside the comparison CI created in the first stage of this paper, which is traditionally considered a small effect size for variance overlap measures [@Cohen1992a]. RMSE values were increased outside the confidence interval for $\omega^2$ and $\epsilon^2$ across most *n* values over 50, averaging `r format(abs(avgrmsehi), nsmall = 3)` over the comparison CI. However, the SD for each effect size was unaffected by the rounding to whole numbers. Because it is difficult to capture such small differences in static graphics, interested readers can view an interactive Shiny application on our OSF page. The Shiny app allows users to view figures in color, as well as manipulate the y-axis. Precise decimals are included with each figure, and while we report RMSE values here outside the CI created to examine changes, a reader can interpret these values with their own criteria online. With these results, we do not suggest that researchers should use less precise dependent variables; however, it is important to consider that effect size estimates from rounded data may be under-representing the size of their population effects and a bit more variable than expected.

## Type of Dependent Variable and Data Skew
  Given that Likert data is generally represented as whole numbers, we examined the effect of Likert type dependent variables may indirectly change effect sizes. Often, Likert scales are criticized for their non-normal distributions [@Bishop2015], where participants will choose only the ends of the scale. Therefore, in this set of simulations we first examined the effect of simply shifting the estimated sample means over to 2, 2, 3.2, and 3.2 to mimic a floor effect on a traditional 1-7 style scale. 1,000 simulations indicated no large differences from the comparison study (part one of this paper), and therefore, all data and graphs are presented online for the interested reader. After confirming that we did not see changes by shifting away from a standard normal distribution, we then simulated 1,000 datasets with adjusted means that were truncated and thus, skewed. This procedure rounded all values less than one to one, and all values over seven down to seven. In each of these simulation tests, we only manipulated one variable, and therefore, the data in this test were otherwise not rounded (i.e., precision was set to default decimals). 
```{r likert, eval=FALSE, include=FALSE}
####Likert data####
muvec <- c(2.00,2.00,3.20,3.20)
meanmu <- mean(muvec)
sigb <- sum((muvec-meanmu)^2)/4
eta2p <- sigb/(sigb+1)
k <- length(muvec)
nsim <- 1000
njs <- c(10,20,30,40,50,60,70,80,90,100)
BIASmat <- matrix(NA,nrow=length(njs),ncol=3)
rownames(BIASmat) <- njs
colnames(BIASmat) <- c("eta2","epsilon2","omega2")
RMSEmat <- matrix(NA,nrow=length(njs),ncol=3)
rownames(RMSEmat) <- njs
colnames(RMSEmat) <- c("eta2","epsilon2","omega2")
SDmat <- matrix(NA,nrow=length(njs),ncol=3)
rownames(SDmat) <- njs
colnames(SDmat) <- c("eta2","epsilon2","omega2")
niter <- 1

for (nj in njs){
  x <- matrix(NA,nrow=nj,ncol=4)
  eta2 <- rep(NA,nsim)
  epsilon2 <- rep(NA,nsim)
  omega2 <- rep(NA,nsim)
  for (ii in 1: nsim){
    y <- c(rnorm(n=nj,mean=muvec[1],sd=1),
           rnorm(n=nj,mean=muvec[2],sd=1),
           rnorm(n=nj,mean=muvec[3],sd=1),
           rnorm(n=nj,mean=muvec[4],sd=1))
    
    x <- as.factor(c(rep("mu1",nj),rep("mu2",nj),
                     rep("mu3",nj),rep("mu4",nj)))
    
    res <- anova(aov(y~x))
    res <- as.matrix(res)
    SSb <- res[1,2]
    SSt <- res[1,2] + res[2,2]
    MSw <- res[2,3]
    
    eta2[ii] <- SSb/SSt
    epsilon2[ii] <- (SSb - 3*MSw)/SSt
    omega2[ii] <- (SSb - 3*MSw)/(SSt+MSw)
  }  
  BIASmat[niter,1] <- mean(eta2) - eta2p
  BIASmat[niter,2] <- mean(epsilon2) - eta2p
  BIASmat[niter,3] <- mean(omega2) - eta2p
  RMSEmat[niter,1] <- sqrt(sum((eta2-eta2p)^2)/nsim)
  RMSEmat[niter,2] <- sqrt(sum((epsilon2-eta2p)^2)/nsim)
  RMSEmat[niter,3] <- sqrt(sum((omega2-eta2p)^2)/nsim)
  SDmat[niter,1] <- sqrt(sum((eta2-mean(eta2))^2)/nsim)
  SDmat[niter,2] <- sqrt(sum((epsilon2-mean(epsilon2))^2)
                         /nsim)
  SDmat[niter,3] <- sqrt(sum((omega2-mean(omega2))^2)
                         /nsim)
  niter <- niter+1
}

write.csv(cbind(BIASmat, RMSEmat, SDmat), "likert.csv")
```

```{r likert-graph, eval=FALSE, include=FALSE, fig.cap="Bias, SD, and RMSE values for Likert style dependent variables. Shaded gray area represents 95% confidence interval around these values.", fig.height=6, fig.width=6}
likert = read.csv("likert.csv")

####bias graph####
bgraphdatalong = melt(likert[ , 1:4],
                      id = "N",
                      measured = c("eta2B", "epsilon2B", "omega2B"))
bgraphdatalong$variable = factor(bgraphdatalong$variable, 
                                 levels = c("epsilon2B", "eta2B", "omega2B"))
bgraphdatalong$HICI = melt(unname(c(hibeta, hibep, hibom)))$value
bgraphdatalong$LOCI = melt(unname(c(lobeta, lobep, lobom)))$value

biasline = 
  ggplot(bgraphdatalong, aes(N, value, 
                             color = variable,
                             group = variable)) +
  geom_line(size=.7) +
  geom_ribbon(aes(ymin = LOCI, 
                  ymax = HICI, 
                  group = variable), 
              fill = "grey70",
              color = FALSE,
              alpha = .4) +
  coord_cartesian(ylim = c(-.02, .14)) +
  ylab("Bias") +
  xlab("Sample Size") + 
  geom_abline(slope = 0, intercept = 0) +
  scale_color_manual(name = "Effect Size", 
                     labels = c("Epsilon", "Eta", "Omega"),
                     values = c("darkgrey", "black", "grey20")) +
  cleanup

####rmse graph####
bgraphdatalong2 = melt(likert[ , c(1, 5:7)],
                      id = "N",
                      measured = c("epsilon2R", "eta2R", "omega2R"))
bgraphdatalong2$variable = factor(bgraphdatalong2$variable, 
                                 levels = c("epsilon2R", "eta2R", "omega2R"))
bgraphdatalong2$HICI = melt(unname(c(hireta, hirep, hirom)))$value
bgraphdatalong2$LOCI = melt(unname(c(loreta, lorep, lorom)))$value

rmseline = 
  ggplot(bgraphdatalong2, aes(N, value, 
                             color = variable,
                             group = variable)) +
  geom_line(size=.7) +
  geom_ribbon(aes(ymin = LOCI, 
                  ymax = HICI, 
                  group = variable), 
              fill = "grey70",
              color = FALSE,
              alpha = .4) +
  coord_cartesian(ylim = c(-.02, .14)) +
  ylab("RMSE") +
  xlab("Sample Size") + 
  geom_abline(slope = 0, intercept = 0) +
  scale_color_manual(values = c("darkgrey", "black", "grey20")) +
  cleanup

####sd graph####
bgraphdatalong3 = melt(likert[ , c(1, 8:10)],
                      id = "N",
                      measured = c("epsilon2S", "eta2S", "omega2S"))
bgraphdatalong3$variable = factor(bgraphdatalong3$variable, 
                                 levels = c("epsilon2S", "eta2S", "omega2S"))
bgraphdatalong3$HICI = melt(unname(c(hiseta, hisep, hisom)))$value
bgraphdatalong3$LOCI = melt(unname(c(loseta, losep, losom)))$value

sdline = 
  ggplot(bgraphdatalong3, aes(N, value, 
                             color = variable,
                             group = variable)) +
  geom_line(size=.7) +
  geom_ribbon(aes(ymin = LOCI, 
                  ymax = HICI, 
                  group = variable), 
              fill = "grey70",
              color = FALSE,
              alpha = .4) +
  coord_cartesian(ylim = c(-.02, .14)) +
  ylab("SD") +
  xlab("Sample Size") + 
  geom_abline(slope = 0, intercept = 0) +
  scale_color_manual(values = c("darkgrey", "black", "grey20")) +
  cleanup

# arrange plots together
library(cowplot)
legend = get_legend(biasline)
prow <- plot_grid( biasline + theme(legend.position="none"),
                   rmseline + theme(legend.position="none"),
                   sdline + theme(legend.position="none"),
                   legend,
                   hjust = -1,
                   nrow = 2
)
prow

bgraphdatalong$outhi = bgraphdatalong$value - bgraphdatalong$HICI
bgraphdatalong$outlo = bgraphdatalong$value - bgraphdatalong$LOCI

bgraphdatalong2$outhi = bgraphdatalong2$value - bgraphdatalong2$HICI
bgraphdatalong2$outlo = bgraphdatalong2$value - bgraphdatalong2$LOCI

bgraphdatalong3$outhi = bgraphdatalong3$value - bgraphdatalong3$HICI
bgraphdatalong3$outlo = bgraphdatalong3$value - bgraphdatalong3$LOCI
```

```{r truncate, eval=FALSE, include=FALSE}
####Likert data + truncation####
muvec <- c(2.00,2.00,3.20,3.20)
meanmu <- mean(muvec)
sigb <- sum((muvec-meanmu)^2)/4
eta2p <- sigb/(sigb+1)
k <- length(muvec)
nsim <- 1000
njs <- c(10,20,30,40,50,60,70,80,90,100)
BIASmat <- matrix(NA,nrow=length(njs),ncol=3)
rownames(BIASmat) <- njs
colnames(BIASmat) <- c("eta2","epsilon2","omega2")
RMSEmat <- matrix(NA,nrow=length(njs),ncol=3)
rownames(RMSEmat) <- njs
colnames(RMSEmat) <- c("eta2","epsilon2","omega2")
SDmat <- matrix(NA,nrow=length(njs),ncol=3)
rownames(SDmat) <- njs
colnames(SDmat) <- c("eta2","epsilon2","omega2")
niter <- 1

for (nj in njs){
  x <- matrix(NA,nrow=nj,ncol=4)
  eta2 <- rep(NA,nsim)
  epsilon2 <- rep(NA,nsim)
  omega2 <- rep(NA,nsim)
  for (ii in 1: nsim){
    y <- c(rnorm(n=nj,mean=muvec[1],sd=1),
           rnorm(n=nj,mean=muvec[2],sd=1),
           rnorm(n=nj,mean=muvec[3],sd=1),
           rnorm(n=nj,mean=muvec[4],sd=1))
    
    y[y > 7] = 7
    y[y < 1] = 1
    
    x <- as.factor(c(rep("mu1",nj),rep("mu2",nj),
                     rep("mu3",nj),rep("mu4",nj)))
    
    res <- anova(aov(y~x))
    res <- as.matrix(res)
    SSb <- res[1,2]
    SSt <- res[1,2] + res[2,2]
    MSw <- res[2,3]
    
    eta2[ii] <- SSb/SSt
    epsilon2[ii] <- (SSb - 3*MSw)/SSt
    omega2[ii] <- (SSb - 3*MSw)/(SSt+MSw)
  }  
  BIASmat[niter,1] <- mean(eta2) - eta2p
  BIASmat[niter,2] <- mean(epsilon2) - eta2p
  BIASmat[niter,3] <- mean(omega2) - eta2p
  RMSEmat[niter,1] <- sqrt(sum((eta2-eta2p)^2)/nsim)
  RMSEmat[niter,2] <- sqrt(sum((epsilon2-eta2p)^2)/nsim)
  RMSEmat[niter,3] <- sqrt(sum((omega2-eta2p)^2)/nsim)
  SDmat[niter,1] <- sqrt(sum((eta2-mean(eta2))^2)/nsim)
  SDmat[niter,2] <- sqrt(sum((epsilon2-mean(epsilon2))^2)
                         /nsim)
  SDmat[niter,3] <- sqrt(sum((omega2-mean(omega2))^2)
                         /nsim)
  niter <- niter+1
}

write.csv(cbind(BIASmat, RMSEmat, SDmat), "likert_trun.csv")
```

```{r likerttrun-graph, echo=FALSE, fig.cap="Bias, SD, and RMSE values for truncated Likert style dependent variables. Shaded gray area represents 95% confidence interval around these values.", fig.height=8, fig.width=8 }
likertT = read.csv("likert_trun.csv")

####bias graph####
bgraphdatalong = melt(likertT[ , 1:4],
                      id = "N",
                      measured = c("eta2B", "epsilon2B", "omega2B"))
bgraphdatalong$variable = factor(bgraphdatalong$variable, 
                                 levels = c("epsilon2B", "eta2B", "omega2B"))
bgraphdatalong$HICI = melt(unname(c(hibeta, hibep, hibom)))$value
bgraphdatalong$LOCI = melt(unname(c(lobeta, lobep, lobom)))$value

biasline = 
  ggplot(bgraphdatalong, aes(N, value, 
                             color = variable,
                             group = variable)) +
  geom_line(size=.7, aes(linetype = variable)) +
  geom_ribbon(aes(ymin = LOCI, 
                  ymax = HICI, 
                  group = variable), 
              fill = "grey70",
              color = FALSE,
              alpha = .4) +
  coord_cartesian(ylim = c(-.02, .14)) +
  ylab("Bias") +
  xlab("Sample Size") + 
  geom_abline(slope = 0, intercept = 0) +
  scale_color_manual(name = "Effect Size", 
                     labels = c("Epsilon", "Eta", "Omega"),
                     values = c("grey20", "grey20", "grey70")) +
  scale_linetype(name = "Effect Size", 
                 labels = c("Epsilon", "Eta", "Omega")) +
  cleanup

####rmse graph####
bgraphdatalong2 = melt(likertT[ , c(1, 5:7)],
                      id = "N",
                      measured = c("epsilon2R", "eta2R", "omega2R"))
bgraphdatalong2$variable = factor(bgraphdatalong2$variable, 
                                 levels = c("epsilon2R", "eta2R", "omega2R"))
bgraphdatalong2$HICI = melt(unname(c(hireta, hirep, hirom)))$value
bgraphdatalong2$LOCI = melt(unname(c(loreta, lorep, lorom)))$value

rmseline = 
  ggplot(bgraphdatalong2, aes(N, value, 
                             color = variable,
                             group = variable)) +
  geom_line(size=.7, aes(linetype = variable)) +
  geom_ribbon(aes(ymin = LOCI, 
                  ymax = HICI, 
                  group = variable), 
              fill = "grey70",
              color = FALSE,
              alpha = .4) +
  coord_cartesian(ylim = c(-.02, .14)) +
  ylab("RMSE") +
  xlab("Sample Size") + 
  geom_abline(slope = 0, intercept = 0) +
  scale_color_manual(name = "Effect Size", 
                     labels = c("Epsilon", "Eta", "Omega"),
                     values = c("grey20", "grey20", "grey70")) +
  scale_linetype(name = "Effect Size", 
                 labels = c("Epsilon", "Eta", "Omega")) +
  cleanup

####sd graph####
bgraphdatalong3 = melt(likertT[ , c(1, 8:10)],
                      id = "N",
                      measured = c("epsilon2S", "eta2S", "omega2S"))
bgraphdatalong3$variable = factor(bgraphdatalong3$variable, 
                                 levels = c("epsilon2S", "eta2S", "omega2S"))
bgraphdatalong3$HICI = melt(unname(c(hiseta, hisep, hisom)))$value
bgraphdatalong3$LOCI = melt(unname(c(loseta, losep, losom)))$value

sdline = 
  ggplot(bgraphdatalong3, aes(N, value, 
                             color = variable,
                             group = variable)) +
  geom_line(size=.7, aes(linetype = variable)) +
  geom_ribbon(aes(ymin = LOCI, 
                  ymax = HICI, 
                  group = variable), 
              fill = "grey70",
              color = FALSE,
              alpha = .4) +
  coord_cartesian(ylim = c(-.02, .14)) +
  ylab("SD") +
  xlab("Sample Size") + 
  geom_abline(slope = 0, intercept = 0) +
  scale_color_manual(name = "Effect Size", 
                     labels = c("Epsilon", "Eta", "Omega"),
                     values = c("grey20", "grey20", "grey70")) +
  scale_linetype(name = "Effect Size", 
                 labels = c("Epsilon", "Eta", "Omega")) +
  cleanup

# arrange plots together
library(cowplot)
legend = get_legend(biasline)
prow <- plot_grid( biasline + theme(legend.position="none"),
                   rmseline + theme(legend.position="none"),
                   sdline + theme(legend.position="none"),
                   legend,
                   hjust = -1,
                   nrow = 2
)
prow

bgraphdatalong$outhi = bgraphdatalong$value - bgraphdatalong$HICI
bgraphdatalong$outlo = bgraphdatalong$value - bgraphdatalong$LOCI
avgbiashi2 =  round(mean(subset(bgraphdatalong$outhi, bgraphdatalong$outhi > 0)), 3)

bgraphdatalong2$outhi = bgraphdatalong2$value - bgraphdatalong2$HICI
bgraphdatalong2$outlo = bgraphdatalong2$value - bgraphdatalong2$LOCI
avgrmsehi2 =  round(mean(subset(bgraphdatalong$outhi, bgraphdatalong$outhi > 0)), 3)
bgraphdatalong3$outhi = bgraphdatalong3$value - bgraphdatalong3$HICI
bgraphdatalong3$outlo = bgraphdatalong3$value - bgraphdatalong3$LOCI

```

  This simulation set showed the opposite effect of rounding, namely that bias was increased with truncated/skewed data, especially at low *n* (10) and upper *n* (above 60), with an average of `r format(abs(avgbiashi2), nsmall = 3)` increase outside the confidence interval. RMSE and SD were similarly increased `r format(abs(avgrmsehi2), nsmall = 3)` for all effect sizes at both low (*n* = 20) and high (*n* = 80+) ends. Figure \@ref(fig:likerttrun-graph) includes the graphs of these results. 

## Changes Across Effect Size
  In summary, our simulations showed that several aspects of the data change the bias and variance of effect sizes compared to a population parameter. These simulations were calculated on the largest effect from the @Okada2013 to show the greatest possible differences, and the population effect size was set to .265. In this section, we expand the original research and our findings to a range of effect sizes. The simulation code was modified to start with a very small effect size, .012, and increase to a very large effect size, .679. We achieved this range by creating a mean vector that started with zero and each subsequent mean was increased by a decimal additive. This decimal additive started at 0.10 and increased to 1.30 in 0.05 increments. Therefore, the first mean vector was (0, 0.1, 0.2, 0.3) and the last mean vector was (0.0, 1.3, 2.6, 3.9). This procedure increased the population effect size estimate approximately .02 to .04 at a time. The upper end of the range of possible effect sizes estimated was chosen so that the effects of Likert truncation/skew could be examined by starting means at 2.0 and staying within a range of 1-7. 

```{r across-effect, eval=FALSE, include=FALSE}
##change at muvec to 2 for likert data
##uncomment windsorizing for truncation

##functions for rmse and sd calculation
rmsefunction = function (x) { sqrt(sum((x-eta2p)^2)/nsim) }
sdfunction = function(x) { sqrt(sum((x-mean(x))^2)/nsim) }

##set a starting distance
decimal = 0.10 
decimalrounds = 24

##set default values
nsim <- 1000
njs <- c(10,20,30,40,50,60,70,80,90,100)
BIASmat <- matrix(NA,nrow=length(njs)*decimalrounds,ncol=8) 
#rownames(BIASmat) <- njs
colnames(BIASmat) <- c("eta2r","eta2not",
                       "epsilon2r","epsilon2not",
                       "omega2r","omega2not", 
                       "eta2p", "N")
RMSEmat <- matrix(NA,nrow=length(njs)*decimalrounds,ncol=8)
#rownames(RMSEmat) <- njs
colnames(RMSEmat) <-  c("eta2r","eta2not",
                       "epsilon2r","epsilon2not",
                       "omega2r","omega2not", 
                       "eta2p", "N")
SDmat <- matrix(NA,nrow=length(njs)*decimalrounds,ncol=8)
#rownames(SDmat) <- njs
colnames(SDmat) <- c("eta2r","eta2not",
                       "epsilon2r","epsilon2not",
                       "omega2r","omega2not", 
                       "eta2p", "N")
niter <- 1

for (i in 1:decimalrounds) { ##decimal loop start
  muvec = seq(from = 0, to = (decimal*3), by = decimal)
  meanmu <- mean(muvec)
  sigb <- sum((muvec-meanmu)^2)/4
  eta2p <- sigb/(sigb+1)
  
  for (nj in njs){ ##n loop start
    x <- matrix(NA,nrow=nj,ncol=4)
    eta2 <- matrix(NA,nrow=nsim,ncol=2)
    epsilon2 <- matrix(NA,nrow=nsim,ncol=2)
    omega2 <- matrix(NA,nrow=nsim,ncol=2)
    
    for (ii in 1: nsim){ ##simulation start
      y <- c(rnorm(n=nj,mean=muvec[1],sd=1),
             rnorm(n=nj,mean=muvec[2],sd=1),
             rnorm(n=nj,mean=muvec[3],sd=1),
             rnorm(n=nj,mean=muvec[4],sd=1))
      
      #y[y > 7] = 7
      #y[y < 1] = 1
      
      for (iii in 2:1) { ##rounding loop start
        y = round(y, digits = (iii-1))
        x <- as.factor(c(rep("mu1",nj),rep("mu2",nj),
                         rep("mu3",nj),rep("mu4",nj)))
        res <- anova(aov(y~x))
        res <- as.matrix(res)
        SSb <- res[1,2]
        SSt <- res[1,2] + res[2,2]
        MSw <- res[2,3]
        
        eta2[ii,iii] <- SSb/SSt
        epsilon2[ii,iii] <- (SSb - 3*MSw)/SSt
        omega2[ii,iii] <- (SSb - 3*MSw)/(SSt+MSw)
        
      } ##round loop end
      
    } ##simulation loop end
    
    BIASmat[niter,1:2] <- apply(eta2, 2, mean) - eta2p
    BIASmat[niter,3:4] <- apply(epsilon2, 2, mean) - eta2p
    BIASmat[niter,5:6] <- apply(omega2, 2, mean) - eta2p
    BIASmat[niter,7] <- eta2p
    BIASmat[niter,8] <- nj
    RMSEmat[niter,1:2] <- apply(eta2, 2, rmsefunction)
    RMSEmat[niter,3:4] <- apply(epsilon2, 2, rmsefunction)
    RMSEmat[niter,5:6] <- apply(omega2, 2, rmsefunction)
    RMSEmat[niter,7] <- eta2p
    RMSEmat[niter,8] <- nj
    SDmat[niter,1:2] <- apply(eta2, 2, sdfunction)
    SDmat[niter,3:4] <- apply(epsilon2, 2, sdfunction)
    SDmat[niter,5:6] <- apply(omega2, 2, sdfunction)
    SDmat[niter,7] <- eta2p
    SDmat[niter,8] <- nj
      
    niter <- niter+1
    
  } ##n loop end
  decimal = decimal + .05
} ##decimal loop end

write.csv(BIASmat, "bias_across.csv")
write.csv(RMSEmat, "rmse_across.csv")
write.csv(SDmat, "sd_across.csv")
```

```{r across-graph-bias, echo=FALSE, fig.cap = "Bias values for eta squared for regular and truncated Likert data (top and bottom) that has been not been rounded or rounded to whole numbers (left and right). Circle size indicates population eta squared values, wherein small dots are small effect sizes and large dots are large effect sizes.", fig.height=8, fig.width=8}

abias = read.csv("bias_across.csv")
abiasL = read.csv("bias_acrossL.csv")

bgraphdatalong = melt(abias, 
                      id = c("N", "eta2p"))
bgraphdatalong$Type = c(
  rep("Eta", nrow(abias)*2),
  rep("Epsilon", nrow(abias)*2),
  rep("Omega", nrow(abias)*2)
  )
bgraphdatalong$source = c(
  rep("Rounded", nrow(abias)),
  rep("Not Rounded", nrow(abias))
)

bgraphdatalongL = melt(abiasL, 
                      id = c("N", "eta2p"))
bgraphdatalongL$Type = c(
  rep("Eta", nrow(abiasL)*2),
  rep("Epsilon", nrow(abiasL)*2),
  rep("Omega", nrow(abiasL)*2)
  )
bgraphdatalongL$source = c(
  rep("Rounded", nrow(abiasL)),
  rep("Not Rounded", nrow(abiasL))
)

bgraphNR = subset(bgraphdatalong, source == "Not Rounded" & Type == "Eta")
bgraphRR = subset(bgraphdatalong, source == "Rounded" & Type == "Eta")
bgraphNL = subset(bgraphdatalongL, source == "Not Rounded" & Type == "Eta")
bgraphRL = subset(bgraphdatalongL, source == "Rounded" & Type == "Eta")

biaslineNR = 
  ggplot(bgraphNR, aes(N, value, size = eta2p)) +
  geom_point(alpha = 1/5) +
  coord_cartesian(ylim = c(-.03, .08)) +
  ylab("Not Rounded") +
  xlab("Sample Size") + 
  geom_abline(slope = 0, intercept = 0) +
  cleanup +
  scale_size(name = "Pop Eta")


biaslineRR = 
  ggplot(bgraphRR, aes(N, value, size = eta2p)) +
  geom_point(alpha = 1/5) +
  coord_cartesian(ylim = c(-.03, .08)) +
  ylab("Rounded") +
  xlab("Sample Size") + 
  geom_abline(slope = 0, intercept = 0) +
  cleanup +
  scale_size(name = "Pop Eta")
  
biaslineNL = 
  ggplot(bgraphNL, aes(N, value, size = eta2p)) +
  geom_point(alpha = 1/5) +
  coord_cartesian(ylim = c(-.03, .08)) +
  ylab("Not Rounded Likert") +
  xlab("Sample Size") + 
  geom_abline(slope = 0, intercept = 0) +
  cleanup +
  scale_size(name = "Pop Eta")

biaslineRL = 
  ggplot(bgraphRL, aes(N, value, size = eta2p)) +
  geom_point(alpha = 1/5) +
  coord_cartesian(ylim = c(-.03, .08)) +
  ylab("Rounded Likert") +
  xlab("Sample Size") + 
  geom_abline(slope = 0, intercept = 0) +
  cleanup +
  scale_size(name = "Pop Eta")


# arrange plots together
library(cowplot)
legend = get_legend(biaslineNR)
prow <- plot_grid( biaslineNR + theme(legend.position="none"),
                   biaslineRR + theme(legend.position="none"),
                   biaslineNL + theme(legend.position="none"),
                   biaslineRL + theme(legend.position="none"),
                   hjust = -1,
                   nrow = 2
)
prow
```

```{r across-graph-rmse, echo=FALSE, fig.cap = "RMSE values for eta squared for regular and truncated Likert data (top and bottom) that has been not been rounded or rounded to whole numbers (left and right). Circle size indicates population eta squared values, wherein small dots are small effect sizes and large dots are large effect sizes.", fig.height=8, fig.width=8}

armse = read.csv("rmse_across.csv")
armseL = read.csv("rmse_acrossL.csv")

bgraphdatalong = melt(armse, 
                      id = c("N", "eta2p"))
bgraphdatalong$Type = c(
  rep("Eta", nrow(armse)*2),
  rep("Epsilon", nrow(armse)*2),
  rep("Omega", nrow(armse)*2)
  )
bgraphdatalong$source = c(
  rep("Rounded", nrow(armse)),
  rep("Not Rounded", nrow(armse))
)

bgraphdatalongL = melt(armseL, 
                      id = c("N", "eta2p"))
bgraphdatalongL$Type = c(
  rep("Eta", nrow(armseL)*2),
  rep("Epsilon", nrow(armseL)*2),
  rep("Omega", nrow(armseL)*2)
  )
bgraphdatalongL$source = c(
  rep("Rounded", nrow(armseL)),
  rep("Not Rounded", nrow(armseL))
)

bgraphNR = subset(bgraphdatalong, source == "Not Rounded" & Type == "Eta")
bgraphRR = subset(bgraphdatalong, source == "Rounded" & Type == "Eta")
bgraphNL = subset(bgraphdatalongL, source == "Not Rounded" & Type == "Eta")
bgraphRL = subset(bgraphdatalongL, source == "Rounded" & Type == "Eta")

rmselineNR = 
  ggplot(bgraphNR, aes(N, value, size = eta2p)) +
  geom_point(alpha = 1/5) +
  coord_cartesian(ylim = c(.00, .14)) +
  ylab("Not Rounded") +
  xlab("Sample Size") + 
  geom_abline(slope = 0, intercept = 0) +
  cleanup +
  scale_size(name = "Pop Eta")


rmselineRR = 
  ggplot(bgraphRR, aes(N, value, size = eta2p)) +
  geom_point(alpha = 1/5) +
  coord_cartesian(ylim = c(.00, .14)) +
  ylab("Rounded") +
  xlab("Sample Size") + 
  geom_abline(slope = 0, intercept = 0) +
  cleanup +
  scale_size(name = "Pop Eta")
  
rmselineNL = 
  ggplot(bgraphNL, aes(N, value, size = eta2p)) +
  geom_point(alpha = 1/5) +
  coord_cartesian(ylim = c(.00, .14)) +
  ylab("Not Rounded Likert") +
  xlab("Sample Size") + 
  geom_abline(slope = 0, intercept = 0) +
  cleanup +
  scale_size(name = "Pop Eta")

rmselineRL = 
  ggplot(bgraphRL, aes(N, value, size = eta2p)) +
  geom_point(alpha = 1/5) +
  coord_cartesian(ylim = c(.00, .14)) +
  ylab("Rounded Likert") +
  xlab("Sample Size") + 
  geom_abline(slope = 0, intercept = 0) +
  cleanup +
  scale_size(name = "Pop Eta")


# arrange plots together
library(cowplot)
legend = get_legend(biaslineNR)
prow <- plot_grid( rmselineNR + theme(legend.position="none"),
                   rmselineRR + theme(legend.position="none"),
                   rmselineNL + theme(legend.position="none"),
                   rmselineRL + theme(legend.position="none"),
                   hjust = -1,
                   nrow = 2
)
prow

library(data.table)
bgraphNR = as.data.table(bgraphNR)
avgNRrmse = round(mean(bgraphNR[,.SD[which.max(value)],by=N]$eta2p),3)

bgraphRR = as.data.table(bgraphRR)
avgRRrmse = round(mean(bgraphRR[,.SD[which.max(value)],by=N]$eta2p),3)

bgraphNL = as.data.table(bgraphNL)
avgNLrmse = round(mean(bgraphNL[,.SD[which.max(value)],by=N]$eta2p),3)

bgraphRL = as.data.table(bgraphRL)
avgRLrmse = round(mean(bgraphRL[,.SD[which.max(value)],by=N]$eta2p),3)
```
  
```{r across-graph-sd, echo=FALSE, fig.cap = "SD values for eta squared for regular and truncated Likert data (top and bottom) that has been not been rounded or rounded to whole numbers (left and right). Circle size indicates population eta squared values, wherein small dots are small effect sizes and large dots are large effect sizes.", fig.height=8, fig.width=8}

asd = read.csv("sd_across.csv")
asdL = read.csv("sd_acrossL.csv")

bgraphdatalong = melt(asd, 
                      id = c("N", "eta2p"))
bgraphdatalong$Type = c(
  rep("Eta", nrow(asd)*2),
  rep("Epsilon", nrow(asd)*2),
  rep("Omega", nrow(asd)*2)
  )
bgraphdatalong$source = c(
  rep("Rounded", nrow(asd)),
  rep("Not Rounded", nrow(asd))
)

bgraphdatalongL = melt(asdL, 
                      id = c("N", "eta2p"))
bgraphdatalongL$Type = c(
  rep("Eta", nrow(asdL)*2),
  rep("Epsilon", nrow(asdL)*2),
  rep("Omega", nrow(asdL)*2)
  )
bgraphdatalongL$source = c(
  rep("Not Rounded", nrow(asdL)),
  rep("Rounded", nrow(asdL))
)

bgraphNR = subset(bgraphdatalong, source == "Not Rounded" & Type == "Eta")
bgraphRR = subset(bgraphdatalong, source == "Rounded" & Type == "Eta")
bgraphNL = subset(bgraphdatalongL, source == "Not Rounded" & Type == "Eta")
bgraphRL = subset(bgraphdatalongL, source == "Rounded" & Type == "Eta")

sdlineNR = 
  ggplot(bgraphNR, aes(N, value, size = eta2p)) +
  geom_point(alpha = 1/5) +
  coord_cartesian(ylim = c(.00, .14)) +
  ylab("Not Rounded") +
  xlab("Sample Size") + 
  geom_abline(slope = 0, intercept = 0) +
  cleanup +
  scale_size(name = "Pop Eta")


sdlineRR = 
  ggplot(bgraphRR, aes(N, value, size = eta2p)) +
  geom_point(alpha = 1/5) +
  coord_cartesian(ylim = c(.00, .14)) +
  ylab("Rounded") +
  xlab("Sample Size") + 
  geom_abline(slope = 0, intercept = 0) +
  cleanup +
  scale_size(name = "Pop Eta")
  
sdlineNL = 
  ggplot(bgraphNL, aes(N, value, size = eta2p)) +
  geom_point(alpha = 1/5) +
  coord_cartesian(ylim = c(.00, .14)) +
  ylab("Not Rounded Likert") +
  xlab("Sample Size") + 
  geom_abline(slope = 0, intercept = 0) +
  cleanup +
  scale_size(name = "Pop Eta")

sdlineRL = 
  ggplot(bgraphRL, aes(N, value, size = eta2p)) +
  geom_point(alpha = 1/5) +
  coord_cartesian(ylim = c(.00, .14)) +
  ylab("Rounded Likert") +
  xlab("Sample Size") + 
  geom_abline(slope = 0, intercept = 0) +
  cleanup +
  scale_size(name = "Pop Eta")


# arrange plots together
library(cowplot)
legend = get_legend(sdlineNR)
prow <- plot_grid( sdlineNR + theme(legend.position="none"),
                   sdlineRR + theme(legend.position="none"),
                   sdlineNL + theme(legend.position="none"),
                   sdlineRL + theme(legend.position="none"),
                   hjust = -1,
                   nrow = 2
)
prow

library(data.table)
bgraphNR = as.data.table(bgraphNR)
avgNRsd = round(mean(bgraphNR[,.SD[which.max(value)],by=N]$eta2p),3)

bgraphRR = as.data.table(bgraphRR)
avgRRsd = round(mean(bgraphRR[,.SD[which.max(value)],by=N]$eta2p),3)

bgraphNL = as.data.table(bgraphNL)
avgNLsd = round(mean(bgraphNL[,.SD[which.max(value)],by=N]$eta2p),3)

bgraphRL = as.data.table(bgraphRL)
avgRLsd = round(mean(bgraphRL[,.SD[which.max(value)],by=N]$eta2p),3)
```

  Because of the complexity of this data, only $\eta^2$ values were plotted to show the effect of rounding and truncation/skew across a large range of effect sizes. $\eta^2$ was chosen as it is the most commonly reported effect size for ANOVA type designs and the effect size of choice for social scientists [@Lakens2013]. Effects of rounding and truncation/skew are similar across different effect size types. Interested readers can change code in *R*markdown document or view these values for $\omega^2$ and $\epsilon^2$ in our Shiny app under the "Range of Effects" tab. Figure \@ref(fig:across-graph-bias) indicates the bias of $\eta^2$, and the effect of rounding can be seen as a wider spread across the range of sample sizes as compared to non-rounded data that converged across sample size. Truncated/Skewed Likert data showed a similar pattern with slightly less spread than regular data, as well as a slight upward increase in bias, as shown above. Figure \@ref(fig:across-graph-rmse) portrays the subtle effect of rounding and truncation/skew increasing values at the low and high ends of sample size. A new finding shown here was that RMSE appears to be lowest at low and high population effect sizes peaking at $M_{NZ}$ = `r avgNRrmse`, $M_{RZ}$ = `r avgRRrmse`, $M_{NL}$ = `r avgNLrmse`, and $M_{RL}$ = `r avgRLrmse` (with the subscript N for not rounded data, R for rounded data, Z for normal data, and L for truncated Likert data). SD calculation shown in Figure \@ref(fig:across-graph-sd) displays the same pattern with similar average peak values: $M_{NZ}$ = `r avgNRsd`, $M_{RZ}$ = `r avgRRsd`, $M_{NL}$ = `r avgNLsd`, and $M_{RL}$ = `r avgRLsd`.

## Sample Size Planning

```{r power, message=FALSE, warning=FALSE, include=FALSE}
library(pwr)

################### load in bias across
bias_across <- read.csv("bias_across.csv")
#names(bias_across)
#just focus on eta
bias_across = bias_across[,-c(3:6)]
colnames(bias_across) = c("Bias2r","Bias2not","popeta","N")

##add bias with pop to get overall original eta again
bias_across$Eta2r = bias_across$Bias2r + bias_across$popeta
bias_across$Eta2not = bias_across$Bias2not + bias_across$popeta

#add in observed power 2r
bias_across$Power2r = "NA"
nsim = nrow(bias_across)
for(i in 1:nsim){
  n = bias_across$N[i]
  f = sqrt(bias_across$Eta2r[i]/(1-bias_across$Eta2r[i]))
  pow = pwr.anova.test(k = 4, n = n, f = f, sig.level = 0.05)
  bias_across$Power2r[i] = pow$power
}

#add in n estimate for each eta 2r
bias_across$Nest2r = "NA"
for(i in 1:nsim){ 
  f = sqrt(bias_across$Eta2r[i]/(1-bias_across$Eta2r[i]))
  pow = pwr.anova.test(k=4, f = f, power = .8, sig.level = 0.05)
  bias_across$Nest2r[i] = pow$n
} 

# Nsample - Npop 2r
bias_across$popNest = "NA"
k=4 
for(i in 1:nsim){
  f = sqrt(bias_across$popeta[i] / (1-bias_across$popeta[i]))
  powtest = pwr.anova.test(k=k,f=f,power=.8,sig.level=0.05)
  bias_across$popNest[i] = powtest$n
}

bias_across$Ndiff2r = as.numeric(bias_across$Nest2r) - 
  as.numeric(bias_across$popNest)

#add in observed power 2not
bias_across$Power2not = "NA"
nsim = nrow(bias_across)
for(i in 1:nsim){
  n = bias_across$N[i]
  f = sqrt(bias_across$Eta2not[i]/(1-bias_across$Eta2not[i]))
  pow = pwr.anova.test(k = 4, n = n, f = f, sig.level = 0.05)
  bias_across$Power2not[i] = pow$power
}
#add in n estimate for each eta 2not
bias_across$Nest2not = "NA"
for(i in 1:nsim){
  f = sqrt(bias_across$Eta2not[i]/(1-bias_across$Eta2not[i]))
  pow = pwr.anova.test(k=4, f = f, power = .8, sig.level = 0.05)
  bias_across$Nest2not[i] = pow$n
} 

# Nsample - Npop 2not
bias_across$Ndiff2not = as.numeric(bias_across$Nest2not) -
  as.numeric(bias_across$popNest) 
```

```{r powerL-graph, echo=FALSE, fig.cap = "Sample size differences estimated for 80% power for found versus population effect sizes. Negative values indicate an underestimation of sample size needed to detect a significant effect. Circle size indicates population eta squared values, wherein small dots are small effect sizes and large dots are large effect sizes. The left panel indicates the full range of the data, while the right panel is zoomed in to show the effects at larger population values.", fig.height=8, fig.width=8}

################### load in bias across L
bias_acrossL <- read.csv("bias_acrossL.csv")
#names(bias_acrossL)
#just focus on eta
bias_acrossL = bias_acrossL[,-c(3:6)]
colnames(bias_acrossL) = c("Bias2r","Bias2not","popeta","N")
bias_acrossL$Eta2r = bias_acrossL$Bias2r + bias_acrossL$popeta
bias_acrossL$Eta2not = bias_acrossL$Bias2not + bias_acrossL$popeta
#add in observed power 2r
bias_acrossL$Power2r = "NA"
nsim = nrow(bias_acrossL)
for(i in 1:nsim){
  n = bias_acrossL$N[i]
  f = sqrt(bias_acrossL$Eta2r[i]/(1-bias_acrossL$Eta2r[i]))
  pow = pwr.anova.test(k = 4, n = n, f = f, sig.level = 0.05)
  bias_acrossL$Power2r[i] = pow$power
}
#add in n estimate for each eta 2r
bias_acrossL$Nest2r = "NA"
for(i in 1:nsim){
  f = sqrt(bias_acrossL$Eta2r[i]/(1-bias_acrossL$Eta2r[i]))
  pow = pwr.anova.test(k=4, f = f, power = .8, sig.level = 0.05)
  bias_acrossL$Nest2r[i] = pow$n
} 

# Nsample - Npop 2r
bias_acrossL$popNest = "NA"
k=4 
for(i in 1:nsim){
  f = sqrt(bias_acrossL$popeta[i] / (1 - bias_acrossL$popeta[i]))
  powtest = pwr.anova.test(k=k,f=f,power=.8,sig.level=0.05)
  bias_acrossL$popNest[i] = powtest$n
}
bias_acrossL$Ndiff2r = as.numeric(bias_acrossL$Nest2r) - as.numeric(bias_acrossL$popNest)

#add in observed power 2not
bias_acrossL$Power2not = "NA"
nsim = nrow(bias_acrossL)
for(i in 1:nsim){
  n = bias_acrossL$N[i]
  f = sqrt(bias_acrossL$Eta2not[i]/(1-bias_acrossL$Eta2not[i]))
  pow = pwr.anova.test(k = 4, n = n, f = f, sig.level = 0.05)
  bias_acrossL$Power2not[i] = pow$power
}
#add in n estimate for each eta 2not
bias_acrossL$Nest2not = "NA"
for(i in 1:nsim){
  f = sqrt(bias_acrossL$Eta2not[i]/(1-bias_acrossL$Eta2not[i]))
  pow = pwr.anova.test(k=4, f = f, power = .8, sig.level = 0.05)
  bias_acrossL$Nest2not[i] = pow$n
} 

# Nsample - Npop 2not
bias_acrossL$Ndiff2not = as.numeric(bias_acrossL$Nest2not) - as.numeric(bias_acrossL$popNest)

tempdata = cbind(bias_across[ , c(3,4,10,13)], bias_acrossL[ , c(10,13)])
colnames(tempdata)[5:6] = c("Ndiff2rL", "Ndiff2notL")

longtempdata = melt(tempdata, 
                    id = c("popeta", "N"))
colnames(longtempdata)[3:4] = c("Ntype", "Nest")

##multiply difference scores by 4 to get the difference per group
longtempdata$Nest = longtempdata$Nest * 4

powerscatter = 
  ggplot(longtempdata, aes(N, Nest, size = popeta, color = Ntype)) +
  geom_point(alpha = .5) + 
  ylab("N Difference") +
  xlab("Original Sample Size") + 
  geom_abline(slope = 0, intercept = 0) +
  cleanup + 
  scale_color_manual(name = "Data Type",
                     labels = c("Rounded", "Not Rounded", "Likert Rounded", "Likert Not Rounded"),
                     values = c("grey80", "grey60", "grey40", "grey20")) +
  scale_size(name = "Pop Eta") 

powerscatter2 = 
  ggplot(longtempdata, aes(N, Nest, size = popeta, color = Ntype)) +
  geom_point(alpha = .5) + 
  ylab("N Difference") +
  xlab("Original Sample Size") + 
  geom_abline(slope = 0, intercept = 0) +
  cleanup + 
  scale_color_manual(name = "Data Type",
                     labels = c("Rounded", "Not Rounded", "Likert Rounded", "Likert Not Rounded"),
                     values = c("grey80", "grey60", "grey40", "grey20")) +
  scale_size(name = "Pop Eta") +
  coord_cartesian(ylim = c(-200,10))

legend = get_legend(powerscatter2)
powrow <- plot_grid( powerscatter + theme(legend.position="none"),
                      powerscatter2 + theme(legend.position=c(.50,.15)),
                      hjust = -1
)
powrow

MN = tapply(longtempdata$Nest, longtempdata$Ntype, mean)
SDN = tapply(longtempdata$Nest, longtempdata$Ntype, sd)
MinN = tapply(longtempdata$Nest, longtempdata$Ntype, min)
MaxN = tapply(longtempdata$Nest, longtempdata$Ntype, max)

```

This research primarily focused on the effects of changing dependent variables on bias estimates for variance overlap effect sizes. Generally, $\eta^2$ is biased at smaller sample sizes and less biased at larger effect sizes, while $\omega^2$ and $\epsilon^2$ are generally slightly negatively biased. The type of dependent data, rounded and truncated/skewed, can decrease or increase that bias. These values become practically important for sample size planning for future studies. Therefore, we calculated the sample size necessary to attain 80% power with an $\alpha$ of .05 using the *pwr* library [@Champely2016] for both the population and observed $\eta^2$ values for the data calculated in the last section. We focused on $\eta^2$ because of its popularity among researchers, and the differences in bias were apparent across sample size. Again, the interactive Shiny app portrays other effect sizes, color for easier reading, and the ability to zoom the y-axis.  

Figure \@ref(fig:powerL-graph) indicates the proposed sample size difference between population and observed $\eta^2$ across original sample size. The left panel includes the full range of population $\eta^2$, while the right panel zooms in to larger population $\eta^2$ values. All four data types (rounded versus not, Likert versus not) are included in different gray shades, but these estimates are very similar, especially around zero sample size differences. The main takeaway from this graph is the extreme underestimation of suggested sample size at small effect sizes, as they are often the most biased (see Figure \@ref(fig:across-graph-bias)). The average underestimation was close to 30 participants for each type of dependent variable: $M_{NZ}$ = `r apa(MN[2], 2)`, *SD* = `r apa(SDN[2], 2)`; $M_{RZ}$ = `r apa(MN[1], 2)`, *SD* = `r apa(SDN[1], 2)`; $M_{NL}$ = `r apa(MN[4], 2)`, *SD* = `r apa(SDN[4], 2)`; $M_{RL}$ = `r apa(MN[3], 2)`, *SD* = `r apa(SDN[3], 2)`. The minimum values (largest underestimation) were large for small population effect sizes: *NZ* = `r apa(MinN[2],2)`; *RZ* = `r apa(MinN[1],2)`; *NL* = `r apa(MinN[4],2)`; *RL* = `r apa(MinN[3],2)`. The maximum values indicated that sample size is only overestimated in rounded data only: *NZ* = `r apa(MaxN[2],2)`; *RZ* = `r apa(MaxN[1],2)`; *NL* = `r apa(MaxN[4],2)`; *RL* = `r apa(MaxN[3],2)`. Therefore, while effect sizes may not be quite as biased as originally believed, sample size planning was still negatively affected, and it may be prudent to estimate a higher sample size in order to achieve 80% power. 

# Discussion

  Effect sizes are an integral part of statistical inference and are now included in the majority of guidelines for statistical reporting [@Cumming2014]. While the APA Task Force commentary nearly 20 years ago [@Wilkinson1999] has sparked new areas of focus centering on effect sizes, characteristics of different effect sizes have not been as thoroughly explored. $\eta^2$ has dominated research reporting [@Lakens2013], even though alternatives such as $\omega^2$ and $\epsilon^2$ are available and are reportedly better indices of variable overlap [@Okada2013]. $\eta^2$ has been reported to hold a positive bias due to sampling error variability and its dependence on the least-squares method [@Thompson2007; @Snyder1993; @Stevens1992; @Yin2001; @Skidmore2011; @Skidmore2012; @Shieh2008; @Thompson2006]. As sample means deviate from population means, sample based effect sizes deviate from population effects as well [@Maxwell2004]. @Carroll1975 noted that this positive bias is especially prevalent with small *n* and heterogeneous variance in combination. Even though bias-correcting formulas have been proposed to adjust for this bias, a common theme has emerged that $\eta^2$ should be avoided in exchange for less biased effect sizes, like $\omega^2$.
  
  The current paper incorporated aspects from @Okada2013, looking at bias, standard deviation, and standard error for these variance overlap effect size measures. The purpose of the current paper was also to examine the conclusions regarding the utilization of $\eta^2$, as well as examining the type of data used in a given research design to note any changes in performance across $\eta^2$, $\epsilon^2$, and $\omega^2$. This is an important distinction as former studies have investigated the case when the ANOVA model is correct, with conclusions partly ignoring the differential effects found regarding the type of data used. We used 1,000 simulations for our study, which was sufficient to replicate the various effects of bias, standard deviation, and standard error from @Okada2013.
  
  For different types of data, we manipulated the number of decimals (precision of the data), the type of means, and the truncation/skew of simulated data to examine its effects on effect size performance. For each of these, we manipulated one factor at a time, to retain comparability to our confidence intervals created from the first set of simulations. Rounding to zero decimals had an apparent effect on the bias found in all effect sizes, in a downward fashion. Interestingly, we found that less precision of the data resulted in less biased effect sizes, especially after sample sizes for each group crossed 40. This result is in stark contrast to the perception and mantra that $\eta^2$ is *always* biased, and the use of $\omega^2$ and $\epsilon^2$ here would be less biased, but potentially very conservative. The precision of the data may play an important role in understanding if estimates of effect from previous research are useful for thinking of sample size estimation and power.
  
  The next step was to examine Likert style data. As noted earlier, Likert scales can be criticized for the potential of non-normal or skewed distributions [@Bishop2015]. After showing that the location of the scale (i.e., shifting the means up two points) did not make a difference in results, we simulated skew by truncating the simulated data. This manipulation showed an opposite effect of rounding. Bias was shown to increase when using truncated/skewed data, especially at the tails of sample size (*n* = 10, and *n* > 60). RMSE and SD were found to follow the same pattern. These findings were also expanded to consider a range of effect size magnitudes. Figures 4-6 indicate the interactive nature of sample size, population effect size, and choice of data type. Likert scales are still overwhelmingly popular in scientific research, and previous research has mostly focused on the debate of the type (i.e., parametric) of statistics to perform on these scale [@Knapp1990; @Carifio2008]. Here, we demonstrated that choosing these scales can have both and upward and downward bias on effect size depending on the exact scenario the researcher is faced with. However, even as we explored the bias of effect sizes, power analyses showed that we are still underestimating the number of participants needed for adequately powering our studies. 
  
  This conversation about effect size is extremely timely, given the focus on reproducible science [@Nosek2015] and understanding researcher degrees of freedom [@Simmons2011]. @Albers2017 recently shared a preprint examining how the bias of effect sizes and the use of pilot studies can have a confounding effect when estimating sample sizes for well-powered studies. They conclude that the bias of $\eta^2$, as well as *follow-up bias* (i.e., the desire to only explore pilot studies that worked well with larger effects) can create under powered full studies, a significant problem the psychological field has faced in recent years [@Bakker2016]. While suggestions from @Benjamin2017 regarding a change in statistical significance criterions might be a viable short-term solution [however, see @Lakens2017], a bigger problem persists regarding widespread misinterpretation of significance versus strength of effects. As a result, a more in depth examination of effect sizes, its interpretations, and the characteristics in which sample based effects might hold levels of bias is critical. Our simulations indicate this effect is more complicated than a simple understanding of an always positively biased $\eta^2$. An awareness of the type of data researchers use, and ultimately the characteristics of our research design and analysis, is an important step toward a better understanding of the practical importance of psychological effects, and that the effect of data choice should be considered when estimating sample size, considering meta-analyses, or simply interpreting effect sizes. 
  
\newpage

# References

\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}
